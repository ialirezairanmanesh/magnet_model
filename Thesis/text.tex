

بیشتر این  ماژول‌ها در فرایند یادگیری قرار گرفته و کارایی خود را افزایش می‌دهند. عملکرد هر کدام از ماژول‌ها به این شکل است که با انجام یک تبدیل روی داده‌ی ورودی نمایشی از داده را ایجاد کنند که فاکتورهای جداساز بهتری را از داده‌ی خام استخراج کنند. با قراردادن چندین لایه، این مدل قادر به تخمین توابع پیچیده از ورودی‌ها است که هم به کوچکترین جزییات داده حساس بوده و هم به ویژگی‌های غیرمرتبط مانند پس‌زمینه و موقعیت اشیا در تصاویر حساسیتی نداشته باشد. 
یکی از  الگوریتم‌های یادگیری پراستفاده در این شبکه‌ها الگوریتم پس‌انتشار خطا است. این الگوریتم در یک فرایند تکراری و به کمک  روش‌های مبتنی بر مشتقات جزئی بهترین پارامترهای مدل را تعیین می‌کند. الگوریتم پس‌انتشار خطا برای تعیین بهترین پارامترهای هر لایه از شبکه‌ی عصبی عمیق دارای ضعف‌هایی از جمله زمان زیاد آموزش، قرارگرفتن در بهینه‌های محلی، و حساس بودن به پارامتر نرخ یادگیری است. برای حل این ضعف‌ها، راهکارهایی برای استفاده از پارامترهای تصادفی در شبکه‌های عصبی معرفی شد. ماشین یادگیری سریع به عنوان الگوریتمی برای  حل ضعف‌های موجود در الگوریتم پس‌انتشار خطا معرفی شد. این الگوریتم برای آموزش یک شبکه‌ی عصبی با یک لایه‌ی مخفی استفاده ارائه شده است. بدین منظور پارامترهای بین ورودی و لایه‌ی مخفی به صورت تصادفی مقداردهی می‌شوند و پارامترهای بین لایه‌ی مخفی و خروجی مدل براساس کمینه کردن مربعات خطا بین خروجی مدل و خروجی صحیح تعیین می‌شوند.
ساختار تک ‌لایه‌ی این مدل منجر به کاهش کارایی این الگوریتم نسبت به مدل‌های مبتنی بر پس انتشار خطا شده است. بنابراین یکی از چالش‌های مورد بررسی در سال‌های اخیر ایجاد شبکه‌ی عصبی عمیق مبتنی بر ماشین یادگیری سریع، برای افزایش کارایی مدل بوده است. بدین منظور راهکارهای متفاوتی برای ایجاد ساختاری عمیق به کمک ماشین‌یادگیری‌سریع ارائه شده است که دارای نواقصی ازجمله پایداری کم و عدم توجه به یرچسب‌های صحیح داده‌های آموزشی در تعیین پارامترهای هر لایه از مدل است، که کارایی مدل را کاهش می‌دهند. همچنین این مدل‌ها با استخراج ویژگی‌ها از داده‌های خام، تنها قادر به کاهش نویز داده‌ی ورودی هستند و وجود عدم قطعیت در داده‌ها را نمی‌توانند کاهش دهند.


\begin{theorem}
	برای یک مقدار 
	$\epsilon>0$
	و یک تابع فعال سازی  به صورت 
	$\phi : \mathbb{R}\rightarrow \mathbb{R}$
	که در هر بازه بی‌نهایت مشتق پذیر باشد، یک مقدار برای تعداد نرون‌های لایه‌ی مخفی 
	$(L)$
	که کوچکتر از تعداد داده‌های آموزشی 
	$(N)$
	است 
	$(L<N)$
	وجود دارد به طوریکه برای $(N)$ نمونه آموزشی 
\end{theorem}



رای استفاده از این الگوریتم برای آموزش یک شبکه‌ی عصبی چندلایه نیازمند روشی برای تعیین پارامترهای هرلایه از مدل است. یکی از روش‌ها به کارگیری اتوانکدرها برای تعیین پارامترهای هرلایه از مدل است. ساختار یک اتوانکدر درشکل 
\eqref{fig9}
نشان داده شده است. این ساختار مشابه ساختار شبکه‌ی عصبی تک لایه است با این تفاوت که لایه‌ی خروجی برابر با ورودی است و در واقع به دنبال ساخت مجدد داده در خروجی هستیم. 
\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=0.5]{fig9.png}
		\caption{اتوانکدر مبتنی بر ماشین یادگیری سریع }
		\label{fig9}
	\end{center}
\end{figure}
اگر 
$X\in \mathbb{R}^{N\times d}$
داده‌ی ورودی باشد، با مقدار تصادفی به پارامترهای لایه‌ی اول 
$(W,b)$
و محاسبه‌ی پارامترلایه‌ی خروجی 
$(\boldsymbol{\beta})$
براساس آنچه در مورد ماشین یادگیری سریع بیان شد، بهترین پارامترها برای ساخت مجدد داده‌ی ورودی در خروجی محاسبه می‌شود. با استفاده از ترانهاده‌ی ماتریس پارمتر
$(\boldsymbol{\beta})$
به عنوان پارامتر اولین لایه از شبکه‌ی عصبی عمیق، بهترین ویژگی‌ها از داده‌ی خام استخراج می‌شود. با استفاده از یک اتوانکدردر هر لایه  می‌توان پارامترهای هر لایه از یک شبکه‌ی عصبی عمیق را تولید کرد. در انتهای پس از استخراج چندلایه از ویژگی‌ها به کمک الگوریتم ارائه شده برای ماشین یادگیری سریع خروجی آخرین لایه را محاسبه می‌کنیم. شکل 
\eqref{fig10}
نشان دهنده‌ی ساختار تولید یک شبکه‌ی عصبی عمیق به کمک الکوریتم ماشین یادگیری سریع است. 
\begin{figure}[h]
	\begin{center}
		\includegraphics[scale=0.7]{fig10.png}
		\caption{شبکه‌ی عصبی عمیق مبتنی بر اتوانکدر و الگوریتم ماشین یادگیری سریع }
		\label{fig10}
	\end{center}
\end{figure}
\subsection{ماشین یادگیری سریع عمیق }
\subsection{شبکه‌عصبی فازی}


