\vspace {-1.5cm}\textbf {عنوان}~\hfill \textbf {صفحه}\vskip 1mm \hrule height 1.5pt\vspace {.25cm}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2-1}{\ignorespaces ساختار کلی یک شبکه عصبی کانولوشنی (CNN): ابتدا تصویر ورودی یا ماتریس ویژگی وارد شبکه می‌شود. در مرحله استخراج ویژگی، لایه‌های کانولوشنی و تجمعی به ترتیب ویژگی‌های محلی را استخراج و ابعاد داده را کاهش می‌دهند. سپس داده‌ها به یک بردار یک‌بعدی تبدیل شده (Flattening) و وارد لایه‌های کاملاً متصل می‌شوند تا فرایند طبقه‌بندی نهایی انجام گیرد. این ساختار باعث می‌شود شبکه بتواند به صورت خودکار و بدون نیاز به مهندسی ویژگی دستی، الگوهای پیچیده را شناسایی کند. برگرفته از \blx@tocontentsinit {0}\cite {Alsaleh2023}.}}{20}{figure.caption.6}%
\contentsline {figure}{\numberline {2-2}{\ignorespaces نمایی از یک مدل شبکه عصبی گرافی (GNN): ابتدا گراف ورودی (با ویژگی‌های اولیه گره‌ها و ساختار اتصالات) به مدل داده می‌شود. سپس در چندین بلوک GNN (لایه)، اطلاعات گره‌ها با تجمیع اطلاعات از همسایگان و به‌روزرسانی با استفاده از شبکه‌های عصبی کوچک، به‌طور مکرر پالایش می‌شود تا بازنمایی‌های غنی‌تری (گراف تبدیل‌شده) حاصل شود. در نهایت، این بازنمایی‌ها می‌توانند برای وظایف مختلف مانند طبقه‌بندی کل گراف (مثلاً بدافزار/سالم)، طبقه‌بندی گره‌ها (مثلاً شناسایی توابع مخرب) یا پیش‌بینی لبه‌ها استفاده شوند. برگرفته و بازطراحی‌شده بر اساس \blx@tocontentsinit {0}\cite {sanchez-lengeling2021a}.}}{22}{figure.caption.7}%
\contentsline {figure}{\numberline {2-3}{\ignorespaces ساختار کلی معماری ترنسفورمر شامل بخش کدگذار (Encoder) در سمت چپ و گشاینده (Decoder) در سمت راست. هر دو بخش از پشته‌ای از لایه‌های یکسان تشکیل شده‌اند که عمدتاً شامل مکانیزم توجه چندسر (Multi-Head Attention) و شبکه‌های عصبی پیش‌خور (Feed Forward) هستند. اتصالات باقیمانده (Add) و نرمال‌سازی لایه‌ای (Norm) نیز برای پایداری آموزش استفاده می‌شوند. گشاینده علاوه بر توجه خودی، از توجه متقابل (Cross-Attention) برای در نظر گرفتن خروجی کدگذار نیز بهره می‌برد. این معماری امکان پردازش موازی و مدل‌سازی وابستگی‌های بلندمدت را فراهم می‌کند. برگرفته و بازطراحی‌شده بر اساس \blx@tocontentsinit {0}\cite {attention}.}}{25}{figure.caption.8}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3-1}{\ignorespaces معماری مدل MAGNET شامل سه ماژول تخصصی (EnhancedTabTransformer، GraphTransformer، SequenceTransformer)، لایه ادغام چندوجهی و طبقه‌بند باینری.}}{46}{figure.caption.9}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4-1}{\ignorespaces عملکرد هر ماژول (EnhancedTabTransformer، GraphTransformer، SequenceTransformer) را با معیار \lr {F1 Score} نشان می‌دهد. محور افقی ماژول‌ها و محور عمودی مقدار \lr {F1 Score} را نمایش می‌دهد.}}{63}{figure.caption.14}%
\contentsline {figure}{\numberline {4-2}{\ignorespaces روند افزایش \lr {F1 Score} را با افزودن مکانیزم توجه پویا و لایه ادغام چندوجهی نمایش می‌دهد. محور افقی اجزای مدل (بدون توجه پویا، با توجه پویا، با ادغام چندوجهی) و محور عمودی مقدار \lr {F1 Score} را نشان می‌دهد.}}{64}{figure.caption.15}%
\contentsline {figure}{\numberline {4-3}{\ignorespaces تغییرات \lr {F1 Score} و دقت را در طول ۳ دوره آموزش با بهینه‌سازی PIRATES نمایش می‌دهد. محور افقی شماره دوره‌ها و محور عمودی مقادیر \lr {F1 Score} و دقت را نشان می‌دهد.}}{65}{figure.caption.16}%
\addvspace {10\p@ }
\addvspace {10\p@ }
