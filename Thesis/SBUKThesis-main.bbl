\begin{thebibliography}{10}

\bibitem{CoT}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia,
  Ed~Chi, Quoc Le, and Denny Zhou.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models, 2023.

\bibitem{PoT}
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William~W. Cohen.
\newblock Program of thoughts prompting: Disentangling computation from
  reasoning for numerical reasoning tasks, 2023.

\bibitem{PS}
Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and
  Ee-Peng Lim.
\newblock Plan-and-solve prompting: Improving zero-shot chain-of-thought
  reasoning by large language models, 2023.

\bibitem{PromptBreeder}
Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim
  Rocktäschel.
\newblock Promptbreeder: Self-referential self-improvement via prompt
  evolution, 2023.

\bibitem{DPP_for_ML}
Alex Kulesza.
\newblock Determinantal point processes for machine learning.
\newblock {\em Foundations and Trends® in Machine Learning},
  5(2–3):123–286, 2012.

\bibitem{DPP}
Alexei Borodin.
\newblock Determinantal point processes, 2009.

\bibitem{MultiArith}
Subhro Roy and Dan Roth.
\newblock Solving general arithmetic word problems, 2016.

\bibitem{SingleEQ}
Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and
  Siena~Dumas Ang.
\newblock Parsing algebraic word problems into equations.
\newblock {\em Transactions of the Association for Computational Linguistics},
  3:585--597, 2015.

\bibitem{AddSub}
Mohammad~Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman.
\newblock Learning to solve arithmetic word problems with verb categorization.
\newblock In Alessandro Moschitti, Bo~Pang, and Walter Daelemans, editors, {\em
  Proceedings of the 2014 Conference on Empirical Methods in Natural Language
  Processing ({EMNLP})}, pages 523--533, Doha, Qatar, October 2014. Association
  for Computational Linguistics.

\bibitem{SVAMP}
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
\newblock Are nlp models really able to solve simple math word problems?, 2021.

\bibitem{SQA}
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan
  Berant.
\newblock Did aristotle use a laptop? a question answering benchmark with
  implicit reasoning strategies, 2021.

\bibitem{CSQA}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.
\newblock {C}ommonsense{QA}: A question answering challenge targeting
  commonsense knowledge.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio, editors, {\em
  Proceedings of the 2019 Conference of the North {A}merican Chapter of the
  Association for Computational Linguistics: Human Language Technologies,
  Volume 1 (Long and Short Papers)}, pages 4149--4158, Minneapolis, Minnesota,
  June 2019. Association for Computational Linguistics.

\bibitem{AquaRat}
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom.
\newblock Program induction by rationale generation : Learning to solve and
  explain algebraic word problems, 2017.

\bibitem{GSM8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
  Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
  Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems, 2021.

\bibitem{attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need, 2023.

\bibitem{mistral}
Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
  Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel,
  Guillaume Lample, Lucile Saulnier, Lélio~Renard Lavaud, Marie-Anne Lachaux,
  Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,
  and William~El Sayed.
\newblock Mistral 7b, 2023.

\bibitem{ZSL}
Soravit Changpinyo, Wei-Lun Chao, Boqing Gong, and Fei Sha.
\newblock Classifier and exemplar synthesis for zero-shot learning, 2019.

\bibitem{FSL}
Archit Parnami and Minwoo Lee.
\newblock Learning from few examples: A summary of approaches to few-shot
  learning, 2022.

\bibitem{beysian}
Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma.
\newblock An explanation of in-context learning as implicit bayesian inference,
  2022.

\bibitem{LLMzeroshot}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
  Iwasawa.
\newblock Large language models are zero-shot reasoners, 2023.

\bibitem{palm2}
Rohan Anil, Andrew~M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,
  Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,
  Eric Chu, Jonathan~H. Clark, Laurent~El Shafey, Yanping Huang, Kathy
  Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson,
  Sebastian Ruder, Yi~Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,
  Gustavo~Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha,
  James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng,
  Colin Cherry, Christopher~A. Choquette-Choo, Aakanksha Chowdhery, Clément
  Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,
  Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus
  Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari,
  Steven Hand, Hadi Hashemi, Le~Hou, Joshua Howland, Andrea Hu, Jeffrey Hui,
  Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao
  Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine
  Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek
  Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma
  Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John
  Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek,
  Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker
  Riley, Alex~Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee
  Shelby, Ambrose Slone, Daniel Smilkov, David~R. So, Daniel Sohn, Simon
  Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,
  Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan
  Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng,
  Ce~Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu.
\newblock Palm 2 technical report, 2023.

\bibitem{opro}
Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc~V. Le, Denny Zhou, and
  Xinyun Chen.
\newblock Large language models as optimizers, 2024.

\bibitem{auto_cot}
Zhuosheng Zhang, Aston Zhang, Mu~Li, and Alex Smola.
\newblock Automatic chain of thought prompting in large language models, 2022.

\bibitem{sentenceBert}
Nils Reimers and Iryna Gurevych.
\newblock Sentence-{BERT}: Sentence embeddings using {S}iamese {BERT}-networks.
\newblock In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors,
  {\em Proceedings of the 2019 Conference on Empirical Methods in Natural
  Language Processing and the 9th International Joint Conference on Natural
  Language Processing (EMNLP-IJCNLP)}, pages 3982--3992, Hong Kong, China,
  November 2019. Association for Computational Linguistics.

\bibitem{APE}
Yongchao Zhou, Andrei~Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis,
  Harris Chan, and Jimmy Ba.
\newblock Large language models are human-level prompt engineers, 2023.

\bibitem{selfconsistencyimproveschainthought}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed~Chi, Sharan Narang,
  Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language
  models, 2023.

\bibitem{TowardsUnderstandingCoTPrompting}
Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and
  Huan Sun.
\newblock Towards understanding chain-of-thought prompting: An empirical study
  of what matters.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, {\em
  Proceedings of the 61st Annual Meeting of the Association for Computational
  Linguistics (Volume 1: Long Papers)}, pages 2717--2739, Toronto, Canada, July
  2023. Association for Computational Linguistics.

\bibitem{cotcollectionimprovingzeroshot}
Seungone Kim, Se~June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin,
  and Minjoon Seo.
\newblock The cot collection: Improving zero-shot and few-shot learning of
  language models via chain-of-thought fine-tuning, 2023.

\bibitem{revealingmysterychainthought}
Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di~He, and Liwei Wang.
\newblock Towards revealing the mystery behind chain of thought: A theoretical
  perspective, 2023.

\bibitem{textpatternseffectivechain}
Aman Madaan and Amir Yazdanbakhsh.
\newblock Text and patterns: For effective chain of thought, it takes two to
  tango, 2022.

\bibitem{ExploringthePromptSpaceofLLMsthroughEvolutionarySampling}
Martina Saletta and Claudio Ferretti.
\newblock Exploring the prompt space of large language models through
  evolutionary sampling.
\newblock In {\em Proceedings of the Genetic and Evolutionary Computation
  Conference}, GECCO '24, page 1345–1353, New York, NY, USA, 2024.
  Association for Computing Machinery.

\bibitem{epiccosteffectivesearchbasedprompt}
Hamed Taherkhani, Melika Sepindband, Hung~Viet Pham, Song Wang, and Hadi
  Hemmati.
\newblock Epic: Cost-effective search-based prompt engineering of llms for code
  generation, 2024.

\bibitem{pca}
Andrzej Maćkiewicz and Waldemar Ratajczak.
\newblock Principal components analysis (pca).
\newblock {\em Computers and Geosciences}, 19:303--342, 3 1993.

\bibitem{tsne}
Laurens van~der Maaten and Geoffrey Hinton.
\newblock Visualizing data using t-sne.
\newblock {\em Journal of Machine Learning Research}, 9(86):2579--2605, 2008.

\end{thebibliography}
